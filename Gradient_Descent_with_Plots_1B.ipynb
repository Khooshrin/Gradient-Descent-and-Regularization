{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDPkazZbup4j"
      },
      "source": [
        "Initially, we randomly shuffled and then split the given dataset into a training, validation and test set of approximately 70%, 10% and 20% respectively. Each of these were imported as a dataframe. We also performed normalisation on the data which helped us to achieve uniform surface plots and coefficients, and made visualisation of our results easier.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiPDuGcBVlCB"
      },
      "source": [
        "**TRAINING DATA**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYz7Yd4ggFKr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/gibsonjackson/FODSIMG/main/TrainFinal.csv')\n",
        "fd = df\n",
        "m=len(df)\n",
        "df=(df-df.min())/(df.max()-df.min()) #normalising the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gx7lQd3ehIGt"
      },
      "outputs": [],
      "source": [
        "x = df.drop(columns = 'quantitative response of LC50')\n",
        "\n",
        "y = df['quantitative response of LC50']\n",
        "Y = y.tolist() #target values of LC50 from dataset\n",
        "Pred_Y=[0.0]*len(Y) #predicted values of LC50 by regression model\n",
        "\n",
        "x1 = df['MLOGP']\n",
        "X1 = x1.tolist()\n",
        "\n",
        "x2 = df['RDCHI']\n",
        "X2 = x2.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Xk8o6C8w27V"
      },
      "source": [
        "*Plotting the dataset on a 3D Plot and visualisating the points as a scatter plot*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "LcwZo4x4g9ZU",
        "outputId": "f63c1a4d-64ef-430d-cf50-bc624395371e"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(X1,X2,Y)\n",
        "\n",
        "ax.set_xlabel('X Label')\n",
        "ax.set_ylabel('Y Label')\n",
        "ax.set_zlabel('Z Label')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iPz2-6quN2pU"
      },
      "source": [
        "Then, we created a gradient descent regression model of all degrees from 0-9 to predict LC50 value and checked our training error on the training dataset. \n",
        "\n",
        "The theta values (weights) are stored in a matrix of size (degree) * (degree+1), initialised to 0. \n",
        "\n",
        "**Hypothesis function**: Initialises a list to store predicted values of Y to all 0's. The prediction is calculated by running a nested loop of i and j as long as their sum is less than or equal to the degree, and then calculating the prediction at that Y as the product of theta[i][j] and the powers of X1 and X2 at that Y with respect to i and j.\n",
        "\n",
        "**Cost function**: This function calculates the root mean square error of all the entries in the prediction of LC50 (calculated by hypothesis) with respect to the target value.\n",
        "\n",
        "**Gradient Linear Regression function**: This algorithm is run for 10,000 epochs, or until the difference between current cost and last cost is less than a very small fixed value. The weights are updated using the old weights and the learning rate and the cost is calculated at every step and checked with the last cost. The list J is created to store the costs and is appended at every step. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7S85eZHxLL4"
      },
      "source": [
        "*Creating a gradient descent regression model for polynomials of degrees 0-9*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1us05nMhJl9"
      },
      "outputs": [],
      "source": [
        "def hypothesis(X1,X2, theta,degree):\n",
        "  Pred_Y=[0.0]*len(X1)\n",
        "  for y in range(len(Pred_Y)):\n",
        "    for i in range(degree+1):\n",
        "      for j in range(degree+1):\n",
        "        if (i+j>degree):\n",
        "          break\n",
        "        Pred_Y[y] = Pred_Y[y] + (theta[i][j]*pow(X1[y],i)*pow(X2[y],j))\n",
        "  return Pred_Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uT4Mx-GJdkgR"
      },
      "outputs": [],
      "source": [
        "def cost(X1, X2,Y, theta, degree):\n",
        "    y1 = hypothesis(X1,X2,theta,degree)\n",
        "    return sum(np.sqrt((np.array(y1)-np.array(Y))**2))/(2*m)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h377UFKsnb0J"
      },
      "outputs": [],
      "source": [
        "def gradientLinearRegression(X1,X2,Y,degree,alpha,epoch):\n",
        "  theta = [[0.0]*(degree+1) for i in range(degree+1)]\n",
        "  J = []\n",
        "  k = 0\n",
        "  size = len(Y)\n",
        "  while k<epoch:\n",
        "    Pred_Y = hypothesis(X1,X2,theta,degree) #Y_pred matrix from equation using theta1x1+theta2x2+theta3\n",
        "    for i in range(degree+1):\n",
        "      for j in range(degree+1):\n",
        "        if (i+j>degree):\n",
        "          break\n",
        "        x1pow=[eachx**i for eachx in X1]\n",
        "        x2pow=[eachx**j for eachx in X2]\n",
        "        theta[i][j]=theta[i][j]-alpha*sum(np.multiply(np.subtract(Pred_Y,Y),np.transpose(np.multiply(x1pow,np.transpose(x2pow)))))/size\n",
        "\n",
        "    j = cost(X1,X2,Y, theta,degree)\n",
        "    J.append(j)\n",
        "    k+=1\n",
        "    #if (k>1 and J[len(J)-2]-j<1e-6):\n",
        "      #break\n",
        "  return J,theta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijPNEW9PxcP_"
      },
      "source": [
        "*Plotting surface plots of predicted polynomials and calculating training error*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-9VUsDzCsWQ"
      },
      "outputs": [],
      "source": [
        "def diagram(X1,X2,Theta, degree):\n",
        "\n",
        "  X1=np.arange(-1,1,0.01)\n",
        "  X2=np.arange(-1,1,0.01)\n",
        "  fig = plt.figure()\n",
        "  ax = fig.gca(projection='3d')\n",
        "  X,Y=np.meshgrid(X1,X2)\n",
        "  \n",
        "  F=np.zeros(len(X1))\n",
        "  index=0\n",
        "  f = lambda i,j: Theta[i][j]*(X**i)*(Y**j)\n",
        "  for i in range(degree+1):\n",
        "    for j in range(degree+1):\n",
        "      if (i+j>degree):\n",
        "        break\n",
        "      F = F + f(i,j)\n",
        "\n",
        "  ax.set_xlabel('MLOGP')\n",
        "  ax.set_ylabel('RDCHI')\n",
        "  ax.set_zlabel('Quantitative Eroor of LC50')\n",
        "  \n",
        "  Z=np.array(F)\n",
        "  ax.plot_surface(X,Y,Z)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uPdSn5dXaDD"
      },
      "source": [
        "Next, we calculate the training error for all 10 degrees of our models. Simultaneously, we plot the calculated polynomials for each degree. \n",
        "\n",
        "allTheta is a list containing the appended values of weights of all the 10 models, which is displayed below the plots. \n",
        "\n",
        "allJ is the total cost for all the entries in the training data for all the 10 models after performing gradient descent polynomial regression, appended in a list. This is tabulated below the theta values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "j0V4-qQ9sQbM",
        "outputId": "a204be12-ae92-47a3-a8c3-98f50434b488"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from tabulate import tabulate\n",
        "from matplotlib import cm\n",
        "from mpl_toolkits import mplot3d\n",
        "%matplotlib inline\n",
        "\n",
        "allTheta=[]\n",
        "for i in range(10):\n",
        "  J,Theta = gradientLinearRegression(X1,X2,Y,i,0.001,10000)\n",
        "  diagram(X1,X2,Theta,i)\n",
        "  allTheta.append(Theta)\n",
        "\n",
        "\n",
        "print(tabulate(allTheta))\n",
        "\n",
        "allJ=[]\n",
        "for i in range(10):\n",
        "  allJ.append(cost(X1,X2,Y,allTheta[i],i))\n",
        "\n",
        "\n",
        "trainerror = pd.DataFrame(allJ, columns=['Training Error']) \n",
        "print(trainerror) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2awwAOEYNeC"
      },
      "source": [
        "As we can clearly see above, the the training error consistently decreases as the degree of the polynomial increases. This is depicting **overfitting** of the model with higher degree polynomials, where the training data is fit almost perfectly. We will view the impact of the same on testing data as well. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrMaZArqTJsm"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLRLJJnwZKap"
      },
      "source": [
        "Then, we perform gradient descent on the validation data set and calculate the costs for each degree, similar to the process explained above. This yields us the polynomial degree with the lowest error, in this case degree 1 and 2. \n",
        "\n",
        "We will check these on the test data without running gradient descent and then finalise the degree with lowest cost. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lx-aHzZFtvlr"
      },
      "source": [
        "**VALIDATION DATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zu-Po4h30Dlj"
      },
      "outputs": [],
      "source": [
        "dval = pd.read_csv('https://raw.githubusercontent.com/gibsonjackson/FODSIMG/main/Validate.csv')\n",
        "# dval=(dval-dval.mean())/(dval.std())\n",
        "dval = (dval-dval.min())/(dval.max()-dval.min())\n",
        "m=len(dval)\n",
        "\n",
        "yval = dval['quantitative response of LC50']\n",
        "Yval = yval.tolist()\n",
        "matrix_y = [0.0]*len(Yval)\n",
        "\n",
        "xval = dval.drop(columns = 'quantitative response of LC50')\n",
        "\n",
        "xval1 = dval['MLOGP']\n",
        "Xval1 = xval1.tolist()\n",
        "\n",
        "xval2 = dval['RDCHI']\n",
        "Xval2 = xval2.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjveURgJ2q9Q"
      },
      "source": [
        "*Running gradient descent on the validation set*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o1_QJZ1DSi_T",
        "outputId": "8e1a2a59-b3bc-406c-ab6d-610ca480cf82"
      },
      "outputs": [],
      "source": [
        "from tabulate import tabulate\n",
        "allvalTheta=[]\n",
        "for i in range(10):\n",
        "  valJ,valTheta = gradientLinearRegression(Xval1,Xval2,Yval,i,0.001,10000)\n",
        "  allvalTheta.append(valTheta)\n",
        "\n",
        "print(tabulate(allvalTheta))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJlSD2qU2wOP"
      },
      "source": [
        "*Finding errors for all degrees of regression models for validation set and choosing the one with the least error*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9lNe-RxSwU5",
        "outputId": "96d70715-a1b4-41b5-dec7-494d22ddce7b"
      },
      "outputs": [],
      "source": [
        "allvalJ=[]\n",
        "for i in range(10):\n",
        "  allvalJ.append(cost(Xval1,Xval2,Yval,allTheta[i],i))\n",
        "\n",
        "\n",
        "validateerror = pd.DataFrame(allvalJ, columns=['Validation Error']) \n",
        "print(validateerror)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AdM4c_ITLtH"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7x98s5iGt1IC"
      },
      "source": [
        "**TEST DATA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SUgWsMi3xRa"
      },
      "outputs": [],
      "source": [
        "dtest = pd.read_csv('https://raw.githubusercontent.com/gibsonjackson/FODSIMG/main/Test.csv')\n",
        "# dtest=(dtest-dtest.mean())/(dtest.std())\n",
        "dtest = (dtest-dtest.min())/(dtest.max()-dtest.min())\n",
        "\n",
        "m=len(dtest)\n",
        "\n",
        "ytest = dtest['quantitative response of LC50']\n",
        "Ytest = ytest.tolist()\n",
        "matrix_y = [0.0]*len(ytest)\n",
        "\n",
        "xtest = dtest.drop(columns = 'quantitative response of LC50')\n",
        "\n",
        "xtest1 = dtest['MLOGP']\n",
        "Xtest1 = xtest1.tolist()\n",
        "\n",
        "xtest2 = dtest['RDCHI']\n",
        "Xtest2 = xtest2.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCxnbr3a3IZd"
      },
      "source": [
        "*Finding test dataset errors*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zIY3Zpa8Wi5",
        "outputId": "e08ad9bc-b732-4589-b770-5670bd489f31"
      },
      "outputs": [],
      "source": [
        "from tabulate import tabulate\n",
        "import math\n",
        "\n",
        "alltestTheta=[]\n",
        "alltestJ=[]\n",
        "for i in range(10):\n",
        "  alltestJ.append(cost(Xtest1,Xtest2,Ytest,allTheta[i],i))\n",
        "\n",
        "\n",
        "testerror = pd.DataFrame(alltestJ, columns=['Test Error']) \n",
        "print(testerror) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNFLaXWUZtEE"
      },
      "source": [
        "As clearly visible, the costs decrease, reach a minimum value and then proceed to increase. \n",
        "\n",
        "**The lowest cost is gotten at degree 1, and the lowest error is j = 0.069304.**\n",
        "\n",
        "Overfitting occurs at higher degrees, since in the training set, the values of cost keep on decreasing even as we reach degree 9, but in testing (unseen) data, the cost increases. This implies that the model is not able to generalise well. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
