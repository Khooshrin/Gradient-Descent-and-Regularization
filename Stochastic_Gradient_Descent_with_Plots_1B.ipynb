{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJEtYUJj8Og-"
      },
      "source": [
        "Initially, we randomly shuffled and then split the given dataset into a training, validation and test set of approximately 70%, 10% and 20% respectively. Each of these were imported as a dataframe. We also performed normalisation on the data which helped us to achieve uniform surface plots and coefficients, and made visualisation of our results easier.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1eIoukUagH9r"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/gibsonjackson/FODSIMG/main/TrainFinal.csv')\n",
        "fd = df\n",
        "df = (df-df.min())/(df.max()-df.min())\n",
        "Res = df['quantitative response of LC50']\n",
        "X1 = df['MLOGP']\n",
        "X2 = df['RDCHI']\n",
        "Y1 = Res.tolist()\n",
        "X1 = X1.tolist()\n",
        "X2 = X2.tolist()\n",
        "\n",
        "\n",
        "dfv = pd.read_csv('https://raw.githubusercontent.com/gibsonjackson/FODSIMG/main/Validate.csv')\n",
        "dfv = (dfv-dfv.min())/(dfv.max()-dfv.min())\n",
        "fdv = df\n",
        "Resv = dfv['quantitative response of LC50']\n",
        "X1v = dfv['MLOGP']\n",
        "X2v = dfv['RDCHI']\n",
        "Y1v = Resv.tolist()\n",
        "X1v = X1v.tolist()\n",
        "X2v = X2v.tolist()\n",
        "\n",
        "\n",
        "dft = pd.read_csv('https://raw.githubusercontent.com/gibsonjackson/FODSIMG/main/Test.csv')\n",
        "dft = (dft-dft.min())/(dft.max()-dft.min())\n",
        "fdt = df\n",
        "Rest = dft['quantitative response of LC50']\n",
        "X1t = dft['MLOGP']\n",
        "X2t = dft['RDCHI']\n",
        "Y1t = Rest.tolist()\n",
        "X1t = X1t.tolist()\n",
        "X2t = X2t.tolist()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqNA7X0O9Nn5"
      },
      "source": [
        "*Creating a stochastic gradient descent regression model for polynomials of degrees 0-9*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TX_iyfo51bWi"
      },
      "source": [
        "Then, we created a stochastic gradient descent regression model of all degrees from 0-9 to predict LC50 value and checked our training error on the training dataset. \n",
        "\n",
        "The theta values (weights) are stored in a matrix of size (degree) * (degree+1), initialised to 0. \n",
        "\n",
        "**Hypothesis function**: Initialises a list to store predicted values of Y to all 0's. The prediction is calculated by running a nested loop of i and j as long as their sum is less than or equal to the degree, and then calculating the prediction at that Y as the product of theta[i][j] and the powers of X1 and X2 at that Y with respect to i and j.\n",
        "\n",
        "**Cost function**: This function calculates the root mean square error of all the entries in the prediction of LC50 (calculated by hypothesis) with respect to the target value.\n",
        "\n",
        "**Gradient Linear Regression function**: This algorithm is run for 10,000 epochs, or until the difference between current cost and last cost is less than a very small fixed value. The weights are updated using the old weights and the learning rate and the cost is calculated at every step and checked with the last cost. The list J is created to store the costs and is appended at every step. \n",
        "\n",
        "The difference between stochastic and normal gradient descent is that stochastic GD takes one training instance at a time and calculates the gradient based on that. Batch gradient descent calculates the gradient everytime based on the entire training set. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxFH69QZlafQ"
      },
      "outputs": [],
      "source": [
        "def hypothesis(X1,X2, theta,degree):\n",
        "  Pred_Y = [0.0]*len(X1)\n",
        "  for y in range(len(Pred_Y)):\n",
        "    for i in range(degree+1):\n",
        "      for j in range(degree+1):\n",
        "        if (i+j>degree):\n",
        "          break\n",
        "        Pred_Y[y] = Pred_Y[y] + (theta[i][j]*pow(X1[y],i)*pow(X2[y],j))\n",
        "  return Pred_Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGSCZ21LmdOi"
      },
      "outputs": [],
      "source": [
        "def cost(X1,X2,Y,theta,degree):\n",
        "    y1 = hypothesis(X1,X2,theta,degree)\n",
        "    return sum(np.sqrt((y1-np.array(Y))**2))/(2*len(X1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8ujczQpglVI"
      },
      "outputs": [],
      "source": [
        "def stochasticLinearRegression(X1,X2,Y,alpha,epoch,degree):\n",
        "  theta = [ [0.0]*(degree+1) for i in range(degree+1)]\n",
        "  J = []\n",
        "  k = 0\n",
        "  size = len(X1)\n",
        "  x1=X1\n",
        "  x2=X2\n",
        "  po = 0\n",
        "  yu = Y\n",
        "  while k<epoch:\n",
        "    y1 = hypothesis(X1,X2,theta,degree) #Y_pred matrix from equation using theta1x1+theta2x2+theta3\n",
        "    yy = y1\n",
        "    for r in range(size):\n",
        "      for i in range(degree+1):\n",
        "        for j in range(degree+1):\n",
        "          if (i+j>degree):\n",
        "            break\n",
        "          \n",
        "          theta[i][j]=theta[i][j]-alpha*((yy[r]-yu[r])*(x1[r]**i)*(x2[r]**j))/size\n",
        "          \n",
        "\n",
        "    k+=1\n",
        "    J.append(cost(X1,X2,Y,theta,degree))\n",
        "  return J,theta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLcQoyDxTWGn"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVDbM3UD9cFn"
      },
      "source": [
        "**TRAINING DATA**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14o68DRf9dUv"
      },
      "source": [
        "*Plotting surface plots of predicted polynomials and calculating training error*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFx4bU0F4Zr7"
      },
      "source": [
        "Next, we calculate the training error for all 10 degrees of our models. Simultaneously, we plot the calculated polynomials for each degree. \n",
        "\n",
        "allTheta is a list containing the appended values of weights of all the 10 models, which is displayed below the plots. \n",
        "\n",
        "allJ is the total cost for all the entries in the training data for all the 10 models after performing gradient descent polynomial regression, appended in a list. This is tabulated below the theta values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vrz_lGB6MOV"
      },
      "outputs": [],
      "source": [
        "def diagram(X1,X2,Theta, degree):\n",
        "  X1=np.arange(-1,1,0.01)\n",
        "  X2=np.arange(-1,1,0.01)\n",
        "  fig = plt.figure()\n",
        "  ax = fig.gca(projection='3d')\n",
        "  X,Y=np.meshgrid(X1,X2)\n",
        "  F=np.zeros(len(X1))\n",
        "  index=0\n",
        "  f = lambda i,j: Theta[i][j]*(X**i)*(Y**j)\n",
        "  for i in range(degree+1):\n",
        "    for j in range(degree+1):\n",
        "      if (i+j>degree):\n",
        "        break\n",
        "      F = F + f(i,j)\n",
        " \n",
        "  ax.set_xlabel('MLOGP')\n",
        "  ax.set_ylabel('RDCHI')\n",
        "  ax.set_zlabel('Quantitative Eroor of LC50')\n",
        "  Z=np.array(F)\n",
        "  ax.plot_surface(X,Y,Z)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6m5kBiqGnii6",
        "outputId": "a5bcb870-36eb-4075-ab07-2af14cf621e2"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from tabulate import tabulate\n",
        "from matplotlib import cm\n",
        "from mpl_toolkits import mplot3d\n",
        "%matplotlib inline\n",
        "\n",
        "allTheta=[]\n",
        "for i in range(10):\n",
        "  print(i)\n",
        "  J,Theta = stochasticLinearRegression(X1,X2,Y1,0.001,10000,i)\n",
        "  diagram(X1,X2,Theta,i)\n",
        "  allTheta.append(Theta)\n",
        "\n",
        "print(tabulate(allTheta))\n",
        "\n",
        "allJ=[]\n",
        "for i in range(10):\n",
        "  allJ.append(cost(X1,X2,Y1,allTheta[i],i))\n",
        "\n",
        "\n",
        "trainerror = pd.DataFrame(allJ, columns=['Training Error']) \n",
        "print(trainerror) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5u1N7U-QTS0M"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYDdr8vA4T1v"
      },
      "source": [
        "\n",
        "As we can clearly see above, the the training error consistently decreases as the degree of the polynomial increases. This is depicting **overfitting** of the model with higher degree polynomials, where the training data is fit almost perfectly. We will view the impact of the same on testing data as well. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCxzr92y94jX"
      },
      "source": [
        "**VALIDATION DATA**\n",
        "\n",
        "*Running gradient descent on the validation set*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsdDs8T24i4H"
      },
      "source": [
        "Then, we perform gradient descent on the validation data set and calculate the costs for each degree, similar to the process explained above. This yields us the polynomial degree with the lowest error, in this case degree 2. \n",
        "\n",
        "We will check these on the test data without running gradient descent and then finalise the degree with lowest cost. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efygU0yt4T6E",
        "outputId": "8a09ed8f-a3ee-4c6d-aa49-57ee340ae9a2"
      },
      "outputs": [],
      "source": [
        "from tabulate import tabulate\n",
        "allThetav=[]\n",
        "Jval = []\n",
        "for i in range(10):\n",
        "  Jv,Thetav = stochasticLinearRegression(X1v,X2v,Y1v,0.001,10000,i)\n",
        "  Jval.append(Jv[len(Jv)-1])\n",
        "  allThetav.append(Thetav)\n",
        "\n",
        "print(tabulate(allThetav))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_7XO5vW-Cjd"
      },
      "source": [
        "*Finding errors for all degrees of regression models for validation set and choosing the one with the least error*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4nYkmogT512",
        "outputId": "770dbc6d-dec6-4454-daa3-b8c7fb78e90a"
      },
      "outputs": [],
      "source": [
        "allvalJ=[]\n",
        "for i in range(10):\n",
        "  allvalJ.append(cost(X1v,X2v,Y1v,allThetav[i],i))\n",
        "\n",
        "\n",
        "validateerror = pd.DataFrame(allvalJ, columns=['Validation Error']) \n",
        "print(validateerror)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkRZPATWTRjk"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkp9-Am--Mq9"
      },
      "source": [
        "**TEST DATA**\n",
        "\n",
        "*Finding test dataset errors*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UndmfJBj4X9d",
        "outputId": "aee01e8d-3515-46db-b4e3-d769c6a43b68"
      },
      "outputs": [],
      "source": [
        "#This is used to test the model against training dataset\n",
        "from tabulate import tabulate\n",
        "allThetat=[]\n",
        "Jtest = []\n",
        "for i in range(10):\n",
        "  J = cost(X1t,X2t,Y1t,allTheta[i],i)\n",
        "  Jtest.append(J)\n",
        "\n",
        "\n",
        "testerror = pd.DataFrame(Jtest, columns=['Test Error']) \n",
        "print(testerror) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u69QBGAi4q2n"
      },
      "source": [
        "As clearly visible, the costs decrease, reach a minimum value and then proceed to increase. \n",
        "\n",
        "**The lowest cost is gotten at degree 1, and the lowest error is j = 0.06930381901223667.**\n",
        "\n",
        "Overfitting occurs at higher degrees, since in the training set, the values of cost keep on decreasing even as we reach degree 9, but in testing (unseen) data, the cost increases. This implies that the model is not able to generalise well. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
